# Chapter 6. 결정 트리

* 회귀, 다중출력이 가능한 다재다능 머신러닝 알고리즘
* 랜덤 포레스트의 기본 구성 요소

* 결정 트리 훈련, 시각화, 예측 방법 -> 규제 가하기/ 회귀 문제에 적용하기 -> 제약 사항

## 6.1 결정 트리 학습과 시각화
* graphviz 패키지를 사용하여 그래프 시각화 가능

* 특정 기준에 따라 데이터를 구분하는 모델
* 한 번의 분기 때마다 변수 영역을 두 개로 구분
* 질문/정답을 담은 네모 상자를 노드(Node)라고 한다.
* Root Node: 깊이가 0인 맨 꼭대기의 노드
* Leaf Node: 자식 노드를 가지지 않는 노드. 맨 마지막 노드

* 결정 트리의 장점: 데이터 전처리가 거의 필요하지 않다. (normalization 작업 필요 x)

## 6.2 예측하기
* 노드의 sample 속성:
* 노드의 value 속성:
* 노드의 gini 속성: 불순도(impurity) 측정
** 한 노드의 모든 샘플이 같은 클래스에 속해 있다면 이 노드를 순수(gini=0)하다고 한다.
** 지니 불순도 계산법
*** pi,k는 i번째 노드에 있는 훈련 샘플 중 클래스 k에 속한 샘플의 비율.

## 6.3 클래스 확률 추정

## 6.4 CART 훈련 알고리즘
* 특성 k와 임곗값 tk를 사용하여 서브셋으로 나눌 때, 이 값을 어떻게 고를까?
* 크기에 따른 가중치가 적용된 가장 순수한 서브셋으로 나눌 수 있는 (k, tk)짝을 찾는다.
* 최소 비용 함수


## 6.5 계산 복잡도

## 6.6 지니 불순도 또는 엔트로피 
* 엔트로피 불순도 사용 가능
* 무질서함을 측정한 것으로, 안정되고 질서 정연하면 엔트로피가 0에 가깝다.
* = 어떤 세트가 한 클래스의 샘플만 담고 있다면 엔트로피가 0이다.

* 감소되는 엔트로피의 양을 정보 이득 (information gain)이라고 부른다.


## 6.6 규제 매개변수
* 비파라미터 모델: 훈련되기 전에 파라미터 수가 결정되지 않는다
* (파라미터 모델은 미리 정의된 파라미터 수를 가지므로 자유도가 제한되고 과대 적합의 위험이 줄어든다 (과소 적합의 위험은 증가)
* 과대적합을 피하기 위해 결정 트리의 자유도를 제한
* 가지치기 (Pruning)


## 6.7 규제 매개변수

## 6.8 회귀
* 각 노드에서 클래스를 예측하는 대신 어떤 값을 예측한다.

## 6.9 불안정성

## 6.10 연습문제
